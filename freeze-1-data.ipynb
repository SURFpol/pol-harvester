{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/matthijm/surfdrive/POL/scraped/elasticsearch-documents.json') as stream:\n",
    "    documents = json.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAMES = ['figshare', 'hbovpk', 'leraar24', 'stimuleringsmaatregel', 'wur', 'wwmhbo']\n",
    "DATA_ROOT = os.path.join('data', 'freeze-1', 'data', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(collection_name):\n",
    "    path = os.path.join(DATA_ROOT, collection_name, 'with_text.json')\n",
    "    items = []\n",
    "    with open(path) as stream:\n",
    "        for item in json.load(stream):\n",
    "            item['collection_name'] = collection_name\n",
    "            items.append(item)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(chain(*[load_data(name) for name in COLLECTION_NAMES]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count #documents/items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series([len(_['documents']) for _ in items]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten items to documents\n",
    "* Skip documents with empty `text` field\n",
    "* Add `collection_name` text field\n",
    "* Rename `content_type` to `mime_type` if necessary\n",
    "* Add item keywords to document keywords\n",
    "* Add `item_id` for reference purposes\n",
    "* Add `item_url` for refrence purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((len(_['documents']) for _ in items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMANIZED_MIME_TYPES = {\n",
    "    'application/pdf': 'pdf',\n",
    "    'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'powerp.',\n",
    "    'application/vnd.ms-powerpoint': 'powerp.',\n",
    "    'application/msword': 'word',\n",
    "    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'word',\n",
    "    'application/rtf': 'word',\n",
    "    'text/plain': 'word',\n",
    "    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'excel',\n",
    "    'text/html': 'html',\n",
    "    'video': 'video',\n",
    "    'image': 'image',\n",
    "    'application/zip': 'zip',\n",
    "    'audio/mpeg': 'audio',\n",
    "    'application/octet-stream': 'other'\n",
    "}\n",
    "\n",
    "def humanize_mime_type(mime_type):\n",
    "    if 'html' in mime_type:\n",
    "        mime_type = 'text/html'\n",
    "        \n",
    "    if 'video' in mime_type:\n",
    "        mime_type = 'video'\n",
    "        \n",
    "    if 'image' in mime_type:\n",
    "        mime_type = 'image'\n",
    "    \n",
    "    return HUMANIZED_MIME_TYPES[mime_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for item in items:\n",
    "    for doc_index, document in enumerate(item['documents']):\n",
    "        document['collection_name'] = item['collection_name']\n",
    "        \n",
    "        if document['collection_name'] == 'leraar24' and 'html' in document['mime_type']:\n",
    "            document['mime_type'] = 'video'\n",
    "            \n",
    "        if not document['mime_type']:\n",
    "            # Try to infer mime type\n",
    "            if 'youtube' in document['url']:\n",
    "                document['mime_type'] = 'video'\n",
    "            elif 'wurtv' in document['url']:\n",
    "                document['mime_type'] = 'video'\n",
    "            else:\n",
    "                print('ignoring', document['collection_name'], document['id'])\n",
    "                continue\n",
    "        \n",
    "        document['humanized_mime_type'] = humanize_mime_type(document['mime_type'])\n",
    "        document['keywords'] = item.get('keywords', [])\n",
    "        document['item_id'] = item['id']\n",
    "        document['item_url'] = item['url']\n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional filters\n",
    "* Verify language is set\n",
    "* Verify text is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [_ for _ in documents if _['language'] is not None]\n",
    "print(len(documents))\n",
    "documents = [_ for _ in documents if _['text'] is not None]\n",
    "print(len(documents))\n",
    "documents = [_ for _ in documents if not (_['humanized_mime_type'] == 'video' and _['language'] == 'en')]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_documents = []\n",
    "ids = set()\n",
    "for d in documents:\n",
    "    if d['id'] in ids:\n",
    "        continue\n",
    "    unique_documents.append(d)\n",
    "    ids.add(d['id'])\n",
    "len(unique_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = unique_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for unfiltered corpus\n",
    "Below are some plots for the unfiltered corpus, that is: the corpus before we selected the documents that should go into Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_collection_name(name):\n",
    "    if name == 'stimuleringsmaatregel':\n",
    "        return 'stim. reg.'\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'mime_type': _['humanized_mime_type'],\n",
    "        'collection_name': shorten_collection_name(_['collection_name']),\n",
    "        'language': _['language']\n",
    "    }\n",
    "    for _ in documents\n",
    "])\n",
    "\n",
    "print(df['collection_name'].value_counts())\n",
    "num_documents = df['collection_name'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "corpus_counts = df['collection_name'].value_counts().to_frame().rename(columns={'collection_name': '#documents'})\n",
    "sns.barplot(data=corpus_counts, x=corpus_counts.index, y='#documents')\n",
    "plt.title('#Documents per corpus - raw (total: {})'.format(num_documents))\n",
    "# plt.savefig('/Users/matthijm/surfdrive/POL/scraped/raw-document-overview.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mime_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "mime_counts = df['mime_type'].value_counts().to_frame().rename(columns={'mime_type': '#documents'})\n",
    "sns.barplot(data=mime_counts, x=mime_counts.index, y='#documents')\n",
    "plt.title('#Documents per MIME type - raw');\n",
    "plt.savefig('/Users/matthijm/surfdrive/POL/scraped/raw-mime-types-all.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=len(COLLECTION_NAMES),\n",
    "    ncols=1,\n",
    "    figsize=(10, len(COLLECTION_NAMES) * 7)\n",
    ")\n",
    "\n",
    "for collection_name, ax in zip(COLLECTION_NAMES, axes):\n",
    "    collection_name = shorten_collection_name(collection_name)\n",
    "    mime_counts =\\\n",
    "        df.loc[df['collection_name'] == collection_name]['mime_type'].value_counts().to_frame().rename(columns={'mime_type': '#documents'})\n",
    "    sns.barplot(data=mime_counts, x=mime_counts.index, y='#documents', ax=ax)\n",
    "    ax.set_title('#Documents per MIME type for \\'{}\\' - raw'.format(collection_name));\n",
    "    \n",
    "plt.savefig('/Users/matthijm/surfdrive/POL/scraped/raw-mime-types-per-collection.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "mime_counts = df['language'].value_counts().to_frame().rename(columns={'language': '#documents'})\n",
    "sns.barplot(data=mime_counts, x=mime_counts.index, y='#documents')\n",
    "plt.title('#Documents per language - raw');\n",
    "plt.savefig('/Users/matthijm/surfdrive/POL/scraped/raw-languages-all.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data for Elasticsearch ingest\n",
    "* Select only video transcripts, Word & Powerpoint documents and PDFs\n",
    "* Select only Dutch and English documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_documents = []\n",
    "\n",
    "for document in documents:\n",
    "    h_mime_type = document['humanized_mime_type']\n",
    "    if h_mime_type not in ['video', 'word', 'powerp.', 'pdf']:\n",
    "        continue\n",
    "\n",
    "    es_documents.append(document)\n",
    "\n",
    "print(len(es_documents))\n",
    "    \n",
    "es_documents = [_ for _ in es_documents if _['language'] in ['nl', 'en']]\n",
    "len(es_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'mime_type': _['humanized_mime_type'],\n",
    "        'collection_name': shorten_collection_name(_['collection_name']),\n",
    "        'language': _['language']\n",
    "    }\n",
    "    for _ in es_documents\n",
    "])\n",
    "\n",
    "num_documents = df['language'].value_counts()\n",
    "print(num_documents)\n",
    "num_documents.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['collection_name'] == 'wur']['mime_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files going into ES to SURFdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/matthijm/surfdrive/POL/scraped/elasticsearch-documents.json', 'wt') as stream:\n",
    "    json.dump(es_documents, stream, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for documents going into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "corpus_counts = df['collection_name'].value_counts().to_frame().rename(columns={'collection_name': '#documents'})\n",
    "sns.barplot(data=corpus_counts, x=corpus_counts.index, y='#documents')\n",
    "plt.title('#Documents per collection - ES (total: {})'.format(num_documents.sum()));\n",
    "\n",
    "# plt.savefig('/Users/matthijm/surfdrive/POL/scraped/es-document-overview.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIME type overview per collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "mime_counts = df['mime_type'].value_counts().to_frame().rename(columns={'mime_type': '#documents'})\n",
    "sns.barplot(data=mime_counts, x=mime_counts.index, y='#documents')\n",
    "plt.title('#Documents per MIME type - ES');\n",
    "plt.savefig('/Users/matthijm/surfdrive/POL/scraped/es-mime-types-all.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=len(COLLECTION_NAMES),\n",
    "    ncols=1,\n",
    "    figsize=(10, len(COLLECTION_NAMES) * 7)\n",
    ")\n",
    "\n",
    "for collection_name, ax in zip(COLLECTION_NAMES, axes):\n",
    "    collection_name = shorten_collection_name(collection_name)\n",
    "    mime_counts =\\\n",
    "        df.loc[df['collection_name'] == collection_name]['mime_type'].value_counts().to_frame().rename(columns={'mime_type': '#documents'})\n",
    "    sns.barplot(data=mime_counts, x=mime_counts.index, y='#documents', ax=ax)\n",
    "    ax.set_title('#Documents per MIME type for \\'{}\\' - ES'.format(collection_name));\n",
    "    \n",
    "plt.savefig('/Users/matthijm/surfdrive/POL/scraped/es-mime-types-per-collection.png', dpi=150, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List indices for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('es-credentials.json') as stream:\n",
    "    credentials = json.load(stream)\n",
    "\n",
    "URL = credentials['url']\n",
    "AUTH = (credentials['username'], credentials['password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "print(requests.get('{}/{}'.format(URL, '_cat/indices'), auth=AUTH).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new index for freeze-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.put(\n",
    "    '{}/{}'.format(URL, 'freeze-1'),\n",
    "    json={\n",
    "        'mappings': {\n",
    "            '_doc': {\n",
    "                'properties': {\n",
    "                    'title': {'type': 'text'},\n",
    "                    'text': {\n",
    "                        'type': 'object',\n",
    "                        'properties': {\n",
    "                            'en': {\n",
    "                                'type': 'text',\n",
    "                                'analyzer': 'english'\n",
    "                            },\n",
    "                            'nl': {\n",
    "                                'type': 'text',\n",
    "                                'analyzer': 'dutch'\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    'url': {'type': 'text'},\n",
    "                    'keywords': {'type': 'text'},\n",
    "                    'mime_type': {'type': 'text'},\n",
    "                    'humanized_mime_type': {'type': 'text'},\n",
    "                    'item_id': {'type': 'text'},\n",
    "                    'item_url': {'type': 'text'},\n",
    "                    'collection_name': {'type': 'text'}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    auth=AUTH\n",
    ").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete index (CAREFUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "requests.delete('{}/{}'.format(URL, 'freeze-1'), auth=AUTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "for doc in ProgressBar()(es_documents):\n",
    "    clean_text = re.sub(r'\\s+', ' ', doc['text'])\n",
    "\n",
    "    es_doc = copy.deepcopy(doc)\n",
    "    es_doc['text'] = dict()\n",
    "    if es_doc['language'] == 'nl':\n",
    "        es_doc['text']['nl'] = clean_text\n",
    "    elif es_doc['language'] == 'en':\n",
    "        es_doc['text']['en'] = clean_text\n",
    "    else:\n",
    "        raise ValueError('this shouldn not happen')\n",
    "    \n",
    "    url = '{}/freeze-1/_doc/{}'.format(URL, doc['id'])\n",
    "    requests.put(url, auth=AUTH, json=es_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('{}/{}/_doc/{}'.format(URL, 'freeze-1', es_documents[0]['id']), auth=AUTH).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List indices for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "print(requests.get('{}/{}'.format(URL, '_cat/indices'), auth=AUTH).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\n",
    "    '{}/{}'.format(URL, 'freeze-1/_search'),\n",
    "    json={\n",
    "        \"from\" : 0, \"size\" : 10,\n",
    "        'query': {\n",
    "            'multi_match': {\n",
    "                'query': 'gene',\n",
    "                'fields': ['text.en']\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    auth=AUTH\n",
    ").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('{}/{}'.format(URL, 'test/_mapping'), auth=AUTH).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
